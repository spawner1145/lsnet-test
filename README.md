# LSNet ç”»å¸ˆé£æ ¼åˆ†ç±»ä¸èšç±»å·¥ä½œæµ

æœ¬ä»“åº“æä¾›äº†ä¸€å¥—é¢å‘ç”»å¸ˆé£æ ¼ç†è§£çš„ç«¯åˆ°ç«¯æµç¨‹ï¼Œæ¶µç›–æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€æ¨ç†éƒ¨ç½²ä¸èšç±»æ£€ç´¢ã€‚ä»¥ä¸‹å†…å®¹å¸®åŠ©ä½ ç¡®è®¤ä»£ç æ˜¯å¦æ»¡è¶³éœ€æ±‚ï¼Œå¹¶åœ¨ä¸€ä¸ªæ–‡æ¡£ä¸­æŒæ¡å…¨æµç¨‹ä½¿ç”¨æ–¹æ³•ã€‚

## åŠŸèƒ½æ¦‚è§ˆ

- **æ•°æ®å‡†å¤‡**ï¼š`prepare_dataset.py` è‡ªåŠ¨å°†åŸå§‹ç”»å¸ˆæ–‡ä»¶å¤¹åˆ’åˆ†ä¸º ImageFolder ç»“æ„ï¼Œå¹¶ç”Ÿæˆ `class_mapping.csv` ç­‰å…ƒä¿¡æ¯ã€‚
- **æ¨¡å‹è®­ç»ƒ**ï¼š`train_artist_style.py` æ”¯æŒå¤šç§ LSNet ç”»å¸ˆæ¨¡å‹ï¼Œæ”¯æŒå¯¹æ¯”æŸå¤±å¢å¼ºè®­ç»ƒï¼Œè®­ç»ƒç»“æŸè‡ªåŠ¨å¯¼å‡ºç±»åˆ«æ˜ å°„ CSV åŠæ¨¡å‹æƒé‡ã€‚
- **æ¨ç†éƒ¨ç½²**ï¼š`inference_artist.py` åœ¨åˆ†ç±»æ¨¡å¼ä¸‹ä¾èµ–è®­ç»ƒç”Ÿæˆçš„ `class_mapping.csv` è¿›è¡Œæ ‡ç­¾æ˜ å°„ï¼Œå¯é€‰æå–ç‰¹å¾è¿›è¡Œèšç±»æˆ–äºŒè€…åŒæ—¶æ‰§è¡Œã€‚
- **å¤šæ ‡ç­¾ä»»åŠ¡**ï¼š`train_artist_multilabel.py` æ”¯æŒåŸºäºå¤šæ ‡ç­¾ CSV çš„è®­ç»ƒä¸è¯„ä¼°ï¼Œ`predict_artist_multilabel.py` æä¾›æ‰¹é‡æ¨ç†ä¸ç½®ä¿¡åº¦/æ¯”é‡å¯¼å‡ºã€‚
- **å·¥å…·è„šæœ¬**ï¼š`utils.py`ã€`losses.py`ã€`robust_utils.py` ç­‰è¾…åŠ©è®­ç»ƒï¼›`flops.py`ã€`speed.py`ã€`eval.sh` ç­‰ç”¨äºæ€§èƒ½æµ‹ç®—ä¸è¯„æµ‹ã€‚


## ç¯å¢ƒå‡†å¤‡

### Python ä¾èµ–

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install --upgrade pip
pip install -r requirements.txt
```

`requirements.txt` å·²æ¶µç›– `torch`ã€`timm`ã€`torchvision` ç­‰æ ¸å¿ƒä¾èµ–ã€‚è‹¥ä½¿ç”¨ GPUï¼Œè¯·ç¡®ä¿æœ¬åœ° CUDA ä¸ PyTorch ç‰ˆæœ¬åŒ¹é…ã€‚

### æ•°æ®ç›®å½•ç»“æ„

åŸå§‹æ•°æ®éœ€æŒ‰ç…§â€œç”»å¸ˆ -> å¤šå¼ ä½œå“â€æ–¹å¼å­˜æ”¾ï¼š

```
source_dir/
â”œâ”€â”€ artist_A/
â”‚   â”œâ”€â”€ img_001.jpg
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ artist_B/
â”‚   â”œâ”€â”€ 0001.png
â”‚   â”œâ”€â”€ ...
â””â”€â”€ ...
```

## æ­¥éª¤ä¸€ï¼šæ•°æ®å‡†å¤‡

ä½¿ç”¨ `prepare_dataset.py` å°†åŸå§‹æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒ/éªŒè¯é›†ï¼Œå¹¶ç”Ÿæˆæ¨ç†æ‰€éœ€çš„ `class_mapping.csv`ã€‚

```powershell
python prepare_dataset.py ^
  --source-dir D:\datasets\raw_artists ^
  --output-dir D:\datasets\artist_dataset ^
  --val-ratio 0.2 ^
  --min-images 10
```

è„šæœ¬æ‰§è¡Œåå°†å¾—åˆ°ï¼š

```
artist_dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ artist_A/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ artist_A/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ class_mapping.csv       # class_id â†” class_name
â”œâ”€â”€ class_names.json
â”œâ”€â”€ dataset_info.json
â””â”€â”€ split_info.txt
```

> `class_mapping.csv` ä¼šåœ¨è®­ç»ƒä¸åˆ†ç±»æ¨ç†é˜¶æ®µé‡å¤ä½¿ç”¨ï¼Œè¯·å¦¥å–„ä¿ç•™ã€‚

## æ­¥éª¤äºŒï¼šæ¨¡å‹è®­ç»ƒ

è¿è¡Œ `train_artist_style.py` ä»¥è®­ç»ƒ LSNet ç”»å¸ˆæ¨¡å‹ã€‚è„šæœ¬ä¼šåœ¨è¾“å‡ºç›®å½•ä¸­ç”Ÿæˆï¼š

- `checkpoint.pth`ï¼šæœ€æ–°æ¨¡å‹æƒé‡
- `model_best.pth`ï¼šæ€§èƒ½æœ€ä½³æƒé‡
- `class_mapping.csv`ï¼šè®­ç»ƒæ—¶æ ¹æ®æ•°æ®é›†è‡ªåŠ¨å¯¼å‡ºçš„ç±»åˆ«æ˜ å°„
- `train_log.txt` / TensorBoard æ—¥å¿—ç­‰

ç¤ºä¾‹å‘½ä»¤ï¼š

```powershell
python train_artist_style.py ^
  --model lsnet_t_artist ^
  --data-path D:\datasets\artist_dataset ^
  --output-dir D:\experiments\lsnet_t ^
  --batch-size 128 ^
  --epochs 300 ^
  --num_workers 8
```

å¸¸ç”¨å‚æ•°è¯´æ˜ï¼š

- `--model`ï¼šå¯é€‰ `lsnet_t_artist`ã€`lsnet_s_artist`ã€`lsnet_b_artist`ã€`lsnet_l_artist`ï¼Œä½ å¯ä»¥åœ¨`model\lsnet_artist.py`é‡Œé¢è‡ªå·±æ”¹å‚æ•°åŠ é¢„è®¾
  - `lsnet_t_artist`: Tinyæ¨¡å‹ï¼Œå‚æ•°é‡çº¦11.4Mï¼Œé€‚åˆå¿«é€Ÿå®éªŒ
  - `lsnet_s_artist`: Smallæ¨¡å‹ï¼Œå‚æ•°é‡çº¦16.1Mï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡
  - `lsnet_b_artist`: Baseæ¨¡å‹ï¼Œå‚æ•°é‡çº¦23.2Mï¼Œæ›´å¥½çš„æ€§èƒ½
  - `lsnet_l_artist`: Largeæ¨¡å‹ï¼Œå‚æ•°é‡çº¦50M+ï¼Œé€‚åˆå¤§è§„æ¨¡è®­ç»ƒå’Œæ›´é«˜ç²¾åº¦éœ€æ±‚
  - `lsnet_xl_artist`: Extra Largeæ¨¡å‹ï¼Œå‚æ•°é‡çº¦100M+ï¼Œä¸“é—¨ç”¨äºå¤„ç†100ä¸‡+å›¾ç‰‡ã€10ä¸‡+ç±»åˆ«çš„å¤§æ•°æ®é›†
- `--eval-every`ï¼šæ¯éš”å¤šå°‘ä¸ªepochè¿›è¡Œä¸€æ¬¡è¯„ä¼°ï¼ˆé»˜è®¤ï¼š1ï¼Œæ¯epochéƒ½è¯„ä¼°ï¼‰
- `--save-every`ï¼šæ¯éš”å¤šå°‘ä¸ªepochä¿å­˜ä¸€æ¬¡checkpointï¼ˆé»˜è®¤ï¼šNoneï¼Œä»…ä¿å­˜æœ€ç»ˆå’Œæœ€ä½³checkpointï¼‰
- `--finetune`ï¼šåœ¨éªŒè¯é˜¶æ®µå°†å›¾åƒç­‰æ¯”ç¼©æ”¾è‡³è®­ç»ƒåˆ†è¾¨ç‡ï¼Œé€‚ç”¨äºè¿ç§»å­¦ä¹ å¾®è°ƒ
- `--dist-eval`ï¼šåœ¨éªŒè¯é˜¶æ®µå¯ç”¨åˆ†å¸ƒå¼é‡‡æ ·ï¼Œä¾¿äºå¤šå¡åŒæ­¥è¯„ä¼°
- `--resume`ï¼šæ–­ç‚¹ç»­è®­
- `--finetune-from`ï¼šä»…åŠ è½½æŒ‡å®š checkpoint çš„æ¨¡å‹æƒé‡ï¼ˆä¼šå¿½ç•¥ä¼˜åŒ–å™¨ç­‰è®­ç»ƒçŠ¶æ€ï¼‰ï¼Œå¸¸ç”¨äºè¿ç§»å­¦ä¹ ï¼›è‹¥åˆ†ç±»æ•°ä¸ä¸€è‡´ä¼šè‡ªåŠ¨é‡ç½®åˆ†ç±»å¤´
- `--teacher-model` / `--teacher-path`ï¼šé…ç½®è’¸é¦æ•™å¸ˆæ¨¡å‹åŠæƒé‡
- `--contrastive-loss`ï¼šå¯ç”¨ç›‘ç£å¯¹æ¯”æŸå¤±ï¼Œæå‡ç‰¹å¾è¡¨ç¤ºè´¨é‡
- `--contrastive-weight`ï¼šå¯¹æ¯”æŸå¤±æƒé‡ï¼ˆé»˜è®¤0.1ï¼Œ0.0=ç¦ç”¨ï¼Œ1.0=ä¸äº¤å‰ç†µç­‰æƒé‡ï¼‰
- `--contrastive-temperature`ï¼šå¯¹æ¯”æŸå¤±æ¸©åº¦å‚æ•°ï¼ˆé»˜è®¤0.07ï¼Œæ§åˆ¶å­¦ä¹ ä¸¥æ ¼ç¨‹åº¦ï¼‰
- `--use-vq`ï¼šåœ¨å¯¹æ¯”æŸå¤±ä¸­å¯ç”¨å‘é‡é‡åŒ–ï¼ˆVQ-VAEé£æ ¼ï¼‰
- `--vq-num-embeddings`ï¼šVQä»£ç æœ¬å¤§å°ï¼ˆé»˜è®¤256ï¼‰
- `--vq-commitment-cost`ï¼šVQæ‰¿è¯ºæŸå¤±æƒé‡ï¼ˆé»˜è®¤0.25ï¼‰

è®­ç»ƒç»“æŸåï¼Œ`output-dir` ä¸‹çš„ `class_mapping.csv` å°†ä½œä¸ºåç»­åˆ†ç±»æ¨ç†çš„å”¯ä¸€æ ‡ç­¾æ˜ å°„æ–‡ä»¶ã€‚

### å¯¹æ¯”æŸå¤±å¢å¼ºè®­ç»ƒ

ä» v1.1.0 å¼€å§‹ï¼Œ`train_artist_style.py` æ”¯æŒ**ç›‘ç£å¯¹æ¯”æŸå¤±**ï¼Œå¯ä»¥æ˜¾è‘—æå‡ç‰¹å¾è¡¨ç¤ºè´¨é‡å’Œåˆ†ç±»æ€§èƒ½ã€‚

#### å¯¹æ¯”æŸå¤±åŸç†

å¯¹æ¯”æŸå¤±é€šè¿‡ä»¥ä¸‹æ–¹å¼æ”¹å–„æ¨¡å‹ï¼š
- **æ­£æ ·æœ¬æ‹‰è¿‘**ï¼šåŒä¸€ç”»å¸ˆçš„ä½œå“ç‰¹å¾å‘é‡æ›´ç›¸ä¼¼
- **è´Ÿæ ·æœ¬æ¨è¿œ**ï¼šä¸åŒç”»å¸ˆçš„ä½œå“ç‰¹å¾å‘é‡æ›´è¿œç¦»
- **ç‰¹å¾å½’ä¸€åŒ–**ï¼šä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—è·ç¦»
- **è”åˆä¼˜åŒ–**ï¼šä¸äº¤å‰ç†µæŸå¤±ååŒå·¥ä½œ

#### ä½¿ç”¨æ–¹æ³•

```powershell
# æ¨èé…ç½®ï¼šå¯ç”¨å¯¹æ¯”æŸå¤±ï¼Œé»˜è®¤æƒé‡
python train_artist_style.py ^
  --model lsnet_t_artist ^
  --data-path D:\datasets\artist_dataset ^
  --output-dir D:\experiments\lsnet_t_contrastive ^
  --batch-size 128 ^
  --epochs 300 ^
  --contrastive-loss

# è‡ªå®šä¹‰æƒé‡é…ç½®
python train_artist_style.py ^
  --model lsnet_t_artist ^
  --data-path D:\datasets\artist_dataset ^
  --output-dir D:\experiments\lsnet_t_contrastive ^
  --batch-size 128 ^
  --epochs 300 ^
  --contrastive-loss ^
  --contrastive-weight 0.2 ^
  --contrastive-temperature 0.05
```

#### å‚æ•°è¯´æ˜

- `--contrastive-loss`ï¼šå¯ç”¨ç›‘ç£å¯¹æ¯”æŸå¤±
- `--contrastive-weight`ï¼šå¯¹æ¯”æŸå¤±æƒé‡
  - `0.0`ï¼šç¦ç”¨å¯¹æ¯”æŸå¤±ï¼ˆé»˜è®¤ï¼‰
  - `0.1`ï¼šè½»é‡è¾…åŠ©æŸå¤±ï¼ˆæ¨èï¼‰
  - `1.0`ï¼šä¸äº¤å‰ç†µç­‰æƒé‡
  - `>1.0`ï¼šæ›´æ³¨é‡ç‰¹å¾å­¦ä¹ 
- `--contrastive-temperature`ï¼šæ¸©åº¦å‚æ•°
  - `0.05-0.1`ï¼šè¾ƒä¸¥æ ¼çš„å­¦ä¹ 
  - `0.1-0.2`ï¼šè¾ƒå¹³æ»‘çš„å­¦ä¹ 

#### æ•ˆæœé¢„æœŸ

- **ç‰¹å¾è´¨é‡æå‡**ï¼šèšç±»å’Œæ£€ç´¢æ€§èƒ½æ”¹å–„
- **åˆ†ç±»å‡†ç¡®ç‡æå‡**ï¼šå°¤å…¶åœ¨ç»†ç²’åº¦é£æ ¼åŒºåˆ†ä¸Š
- **è®­ç»ƒç¨³å®šæ€§**ï¼šå¯¹æ¯”æŸå¤±æä¾›é¢å¤–çš„æ­£åˆ™åŒ–
- **æ¨ç†å…¼å®¹æ€§**ï¼šä¸å½±å“ç°æœ‰æ¨ç†æµç¨‹

#### å‘é‡é‡åŒ–å¢å¼ºï¼ˆVQ-VAEé£æ ¼ï¼‰

ä» v1.2.0 å¼€å§‹ï¼Œæ”¯æŒåœ¨å¯¹æ¯”æŸå¤±ä¸­é›†æˆ**å‘é‡é‡åŒ–ï¼ˆVector Quantizationï¼‰**ï¼Œå®ç°VQ-VAEé£æ ¼çš„ç¦»æ•£ç‰¹å¾å­¦ä¹ ã€‚

**VQæœºåˆ¶åŸç†ï¼š**
- **ä»£ç æœ¬å­¦ä¹ **ï¼šç»´æŠ¤ä¸€ä¸ªç¦»æ•£çš„å‘é‡é›†åˆï¼ˆcodebookï¼‰
- **é‡åŒ–æ˜ å°„**ï¼šå°†è¿ç»­ç‰¹å¾æ˜ å°„åˆ°æœ€è¿‘çš„ä»£ç æœ¬å‘é‡
- **ç›´é€šæ¢¯åº¦**ï¼šä½¿ç”¨ç›´é€šæ¢¯åº¦ä¼°è®¡è§£å†³ç¦»æ•£å˜é‡çš„æ¢¯åº¦é—®é¢˜
- **è”åˆä¼˜åŒ–**ï¼šåŒæ—¶ä¼˜åŒ–é‡å»ºæŸå¤±ã€VQæŸå¤±å’Œæ‰¿è¯ºæŸå¤±

**ä½¿ç”¨æ–¹æ³•ï¼š**

```powershell
# å¯ç”¨VQå¢å¼ºçš„å¯¹æ¯”å­¦ä¹ 
python train_artist_style.py ^
  --model lsnet_t_artist ^
  --data-path D:\datasets\artist_dataset ^
  --output-dir D:\experiments\lsnet_t_vq ^
  --batch-size 128 ^
  --epochs 300 ^
  --contrastive-loss ^
  --contrastive-weight 0.1 ^
  --use-vq ^
  --vq-num-embeddings 512 ^
  --vq-commitment-cost 0.1
```

**VQå‚æ•°è¯´æ˜ï¼š**
- `--use-vq`ï¼šå¯ç”¨å‘é‡é‡åŒ–
- `--vq-num-embeddings`ï¼šä»£ç æœ¬å¤§å°ï¼ˆå»ºè®®256-1024ï¼Œæ ¹æ®ç±»åˆ«æ•°è°ƒæ•´ï¼‰
- `--vq-commitment-cost`ï¼šæ‰¿è¯ºæŸå¤±æƒé‡ï¼ˆå»ºè®®0.1-0.5ï¼‰

**VQå¢å¼ºæ•ˆæœï¼š**
- **ç¦»æ•£è¡¨ç¤º**ï¼šå­¦ä¹ æ›´ç´§å‡‘çš„ç±»åˆ«åŸå‹
- **èšç±»å‹å¥½**ï¼šç¦»æ•£ç‰¹å¾æ›´é€‚åˆèšç±»ç®—æ³•
- **å­˜å‚¨é«˜æ•ˆ**ï¼šå¯ä»¥ç”¨ç´¢å¼•ä»£æ›¿è¿ç»­å‘é‡
- **æ³›åŒ–èƒ½åŠ›**ï¼šå‡å°‘è¿‡æ‹Ÿåˆé£é™©

### å¤šå¡è®­ç»ƒï¼ˆåˆ†å¸ƒå¼å¯åŠ¨ï¼‰

- `train_artist_style.py` å·²é›†æˆ `torch.distributed`ï¼›`--batch-size` æŒ‡æ¯å¼  GPU çš„ batchï¼Œé‡‡æ ·å™¨ä¼šè‡ªåŠ¨æŒ‰ä¸–ç•Œå¤§å°æ‹†åˆ†ã€‚
- æ¨èä½¿ç”¨ **torchrun**ï¼ˆPyTorchâ‰¥1.10ï¼‰å¯åŠ¨ã€‚å®ƒä¼šä¸ºæ¯ä¸ªè¿›ç¨‹è®¾ç½® `RANK / LOCAL_RANK / WORLD_SIZE`ï¼Œè„šæœ¬ä¼šè¿›å…¥åˆ†å¸ƒå¼æ¨¡å¼ã€‚
- æ³¨æ„ï¼šPyTorch çš„ NCCL åç«¯ä»…åœ¨ Linux/WSL ä¸­æ”¯æŒ GPU é€šä¿¡ï¼ŒåŸç”Ÿ Windows ä¸‹è‹¥ä¸ä½¿ç”¨ WSL ä¼šæŠ¥é”™ï¼›å¦‚å¿…é¡»åœ¨ Windows åŸç”Ÿç¯å¢ƒå®éªŒï¼Œå¯æŠŠ `utils.init_distributed_mode` ä¸­çš„ `args.dist_backend` æ”¹ä¸º `gloo`ï¼ˆä»… CPU é€šä¿¡ï¼‰ã€‚

å•æœºä¸¤å¡ç¤ºä¾‹ï¼ˆåœ¨ WSL æˆ– Linux Shell ä¸‹æ‰§è¡Œï¼‰ï¼š

```bash
torchrun --standalone --nnodes=1 --nproc_per_node=2 train_artist_style.py \
  --model lsnet_t_artist \
  --data-path /mnt/d/datasets/artist_dataset \
  --output-dir /mnt/d/experiments/lsnet_t \
  --batch-size 128 \
  --epochs 400 \
  --num_workers 8 \
  --dist-eval
```

- æƒ³é™å®šå¯è§ GPUï¼Œå¯åœ¨å‘½ä»¤å‰åŠ  `CUDA_VISIBLE_DEVICES=0,1`ã€‚
- æ–­ç‚¹ç»­è®­ç»§ç»­å¤šå¡æ—¶æ·»åŠ  `--resume outputs_artist/checkpoint.pth`ï¼Œæ€» batch å˜åŒ–æ—¶è¯·æŒ‰æ¯”ä¾‹è°ƒèŠ‚ `--lr`ã€‚
- å¤šæœºåœºæ™¯éœ€è¦æŠŠ `torchrun` æ¢æˆå¸¦ `--nnodes`ã€`--node_rank`ã€`--master_addr`ã€`--master_port` çš„å¤šæœºå‚æ•°ï¼Œå¹¶ä¿è¯å„èŠ‚ç‚¹ä¹‹é—´ç½‘ç»œäº’é€šã€‚

## å¤šæ ‡ç­¾é£æ ¼æ··åˆä»»åŠ¡(æœªæµ‹è¯•è¿‡ï¼Œä¸è¦ç”¨)

å½“ä¸€å¹…ä½œå“åŒæ—¶èåˆå¤šä½ç”»å¸ˆé£æ ¼æ—¶ï¼Œå¯ä»¥é€šè¿‡å¤šæ ‡ç­¾è®­ç»ƒ/æ¨ç†è„šæœ¬è·å¾—æ›´ç»†ç²’åº¦çš„ç½®ä¿¡åº¦åˆ†æã€‚

### æ•°æ®æ ‡æ³¨æ ¼å¼(æœªæµ‹è¯•è¿‡ï¼Œä¸è¦ç”¨)

æ–°è„šæœ¬ä½¿ç”¨ CSV æè¿°æ ‡ç­¾ï¼š

```csv
image_path,labels
mix_samples/img_001.jpg,"artist_A,artist_B"
mix_samples/img_002.jpg,"(re:zero:1.4),artist_C"
```

- `image_path` ç›¸å¯¹äº `--data-path` æ ¹ç›®å½•æˆ–ä¸ºç»å¯¹è·¯å¾„ã€‚
- `labels` é»˜è®¤ä½¿ç”¨é€—å·åˆ†éš”ï¼Œå¯é€šè¿‡ `--label-delimiter` è‡ªå®šä¹‰ï¼›å¸¦æƒé‡çš„æ¡ç›®å†™æˆ `(label:weight)`ï¼Œä¾‹å¦‚ `(re:zero:1.4)` ä»£è¡¨ `re:zero` å æ® 1.4 çš„æ··åˆæ¯”é‡ï¼ˆè®­ç»ƒæ—¶ä¼šæŒ‰æ¯”ä¾‹å½’ä¸€åŒ–ï¼‰ã€‚
- è®­ç»ƒé›†ä¸éªŒè¯é›†å„è‡ªå¯¹åº”ä¸€ä»½ CSV æ–‡ä»¶ï¼Œç±»åˆ«é›†åˆåœ¨è®­ç»ƒé›†ä¸­è‡ªåŠ¨æ±‡æ€»å¹¶å¤ç”¨åˆ°éªŒè¯é›†ã€‚
- è‹¥å·²æœ‰â€œå›¾ç‰‡ + åŒå .txtâ€ç»“æ„ï¼Œå¯ä½¿ç”¨ï¼š

  ```powershell
  python tools/generate_multilabel_csv.py ^
    --input-dir D:\datasets\artist_mix ^
    --output-csv annotations/all_multilabel.csv ^
    --recursive
  ```

  è‹¥å¸Œæœ›ä¸€æ­¥ç”Ÿæˆè®­ç»ƒ/éªŒè¯ CSVï¼Œå¹¶æŒ‰æ ‡ç­¾å‡ºç°æ¬¡æ•°è¿‡æ»¤ï¼Œå¯æ”¹ç”¨ï¼š

  ```powershell
  python tools/split_multilabel_dataset.py ^
    --input-dir D:\datasets\artist_mix ^
    --train-csv annotations/train_multilabel.csv ^
    --val-csv annotations/val_multilabel.csv ^
    --val-ratio 0.2 ^
    --min-label-count 5 ^
    --relative-paths ^
    --recursive
  ```

  è¯¥è„šæœ¬ä¼šï¼š

  - æŒ‰ `val-ratio` éšæœºåˆ’åˆ† train / valï¼›
  - ä¸¢å¼ƒåœ¨å…¨å±€å‡ºç°å°‘äº `min-label-count` æ¬¡çš„æ ‡ç­¾ï¼›
  - è¾“å‡ºç¬¦åˆè®­ç»ƒè„šæœ¬è¦æ±‚çš„é€—å·åˆ†éš” + æƒé‡è¯­æ³• CSVã€‚

### å¤šæ ‡ç­¾è®­ç»ƒä¸è¯„ä¼°(æœªæµ‹è¯•è¿‡ï¼Œä¸è¦ç”¨)

```powershell
python train_artist_multilabel.py ^
  --data-path D:\datasets\artist_dataset ^
  --train-ann annotations/train_multilabel.csv ^
  --val-ann annotations/val_multilabel.csv ^
  --model lsnet_t_artist ^
  --output-dir D:\experiments\lsnet_t_multilabel ^
  --batch-size 96 ^
  --epochs 120 ^
  --num_workers 8 ^
  --threshold 0.4
```

è„šæœ¬ç‰¹ç‚¹ï¼š

- è‡ªåŠ¨è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„æ­£è´Ÿæ ·æœ¬æƒé‡ï¼Œå¯¹åº” `BCEWithLogitsLoss(pos_weight=â€¦)`ï¼Œç¼“è§£é•¿å°¾åˆ†å¸ƒã€‚
- è®­ç»ƒã€éªŒè¯é˜¶æ®µç»Ÿä¸€è¾“å‡º micro / macro mAPã€F1ã€Precision/Recall ç­‰å¤šæ ‡ç­¾æŒ‡æ ‡ã€‚
- ç»§ç»­æ”¯æŒ `--finetune-from`ã€`--resume`ã€`--dist-eval` ç­‰å¸¸ç”¨å‚æ•°ã€‚
- ä»ä¼šåœ¨è¾“å‡ºç›®å½•ç”Ÿæˆ `class_mapping.csv`ï¼ˆç±»åˆ«é¡ºåºï¼‰ä¸ `label_stats.csv`ï¼ˆæ¯ä¸ªæ ‡ç­¾çš„æ ·æœ¬æ•°ã€å…¨å±€å æ¯”ä¸å¹³å‡æƒé‡ï¼‰ï¼Œä¾¿äºå¤ç”¨ä¸å®¡è®¡ã€‚

è‹¥ä»…æƒ³æŸ¥çœ‹æŒ‡æ ‡ï¼Œå¯ç›´æ¥è¿è¡Œï¼š

```powershell
python train_artist_multilabel.py ^
  --data-path D:\datasets\artist_dataset ^
  --train-ann annotations/train_multilabel.csv ^
  --val-ann annotations/val_multilabel.csv ^
  --model lsnet_t_artist ^
  --resume D:\experiments\lsnet_t_multilabel\best_checkpoint.pth ^
  --eval
```

### å¤šæ ‡ç­¾æ¨ç†ä¸ç½®ä¿¡åº¦æ¯”é‡(æœªæµ‹è¯•è¿‡ï¼Œä¸è¦ç”¨)

```powershell
python predict_artist_multilabel.py ^
  --checkpoint D:\experiments\lsnet_t_multilabel\best_checkpoint.pth ^
  --class-mapping D:\experiments\lsnet_t_multilabel\class_mapping.csv ^
  --inputs D:\samples\hybrid ^
  --output D:\results\hybrid_confidence.json ^
  --top-k 6 ^
  --threshold 0.05 ^
  --normalize-ratio
```

- `--normalize-ratio` ä¼šæŠŠæ¯å¼ å›¾çš„ Sigmoid ç½®ä¿¡åº¦æ ‡å‡†åŒ–ä¸ºå’Œä¸º 1 çš„æ¯”é‡ï¼Œæ–¹ä¾¿è¯„ä¼°é£æ ¼å æ¯”ï¼›ä¸å¯ç”¨æ—¶ä¿ç•™ç‹¬ç«‹çš„ç½®ä¿¡åº¦æ¦‚ç‡ã€‚
- `--debug-full` å¯åœ¨è¾“å‡º JSON ä¸­é™„å¸¦æ‰€æœ‰ç±»åˆ«çš„åŸå§‹ç½®ä¿¡åº¦çŸ©é˜µã€‚
- å¦‚éœ€é€’å½’æ‰«æå­ç›®å½•ï¼Œå¢åŠ  `--recursive`ã€‚

> ğŸ“Œ **ç½®ä¿¡åº¦å¦‚ä½•ç†è§£ï¼Ÿ** å¤šæ ‡ç­¾æ¨¡å‹è¾“å‡ºçš„æ˜¯æ¯ä¸ªç”»å¸ˆæ ‡ç­¾çš„ Sigmoid æ¦‚ç‡ï¼Œä»£è¡¨â€œè¿™å¼ å›¾æ˜¯å¦å«æœ‰è¯¥é£æ ¼â€çš„ç½®ä¿¡åº¦ï¼›è‹¥æƒ³è¿‘ä¼¼ç†è§£ä¸ºé£æ ¼å æ¯”ï¼Œå¯åœ¨å‰è¿°æ¨ç†è„šæœ¬ä¸­å¼€å¯ `--normalize-ratio` å¯¹æ¦‚ç‡å‘é‡åšå½’ä¸€åŒ–ï¼Œè·å¾—ç›¸å¯¹æ¯”é‡å‚è€ƒã€‚

## æ­¥éª¤ä¸‰ï¼šæ¨ç†ä¸ç‰¹å¾æå–

`inference_artist.py` æ”¯æŒä¸‰ç§æ¨¡å¼ï¼š

- `classify`ï¼šä½¿ç”¨åˆ†ç±»å¤´è¾“å‡ºå‰ Top-K ç”»å¸ˆ
- `cluster`ï¼šä»…æå–ç‰¹å¾å‘é‡ï¼ˆæ— éœ€ CSVï¼‰
- `both`ï¼šåŒæ—¶è¾“å‡ºåˆ†ç±»ç»“æœä¸ç‰¹å¾

### å•å¼ å›¾åƒåˆ†ç±»

```powershell
python inference_artist.py ^
  --mode classify ^
  --model lsnet_t_artist ^
  --checkpoint D:\experiments\lsnet_t\model_best.pth ^
  --class-csv D:\experiments\lsnet_t\class_mapping.csv ^
  --input D:\samples\test.jpg ^
  --output D:\results\single ^
  --class-csv artist_dataset\class_mapping.csv
```

**æ¨ç†å‚æ•°è¯´æ˜ï¼š**

- `--top-k`ï¼šæ˜¾ç¤ºçš„é¢„æµ‹ç»“æœæ•°é‡ï¼ˆé»˜è®¤ï¼š5ï¼‰
- `--threshold`ï¼šæ¦‚ç‡é˜ˆå€¼è¿‡æ»¤ï¼Œåªæ˜¾ç¤ºæ¦‚ç‡â‰¥æ­¤å€¼çš„é¢„æµ‹ç»“æœï¼ˆé»˜è®¤ï¼š0.0ï¼Œä»…å•å›¾æ¨ç†æ”¯æŒï¼‰

**ç¤ºä¾‹ï¼šæ˜¾ç¤ºTop-3ç»“æœï¼Œè¿‡æ»¤ä½ç½®ä¿¡åº¦é¢„æµ‹**

```powershell
python inference_artist.py ^
  --mode classify ^
  --model lsnet_t_artist ^
  --checkpoint D:\experiments\lsnet_t\model_best.pth ^
  --class-csv D:\experiments\lsnet_t\class_mapping.csv ^
  --input D:\samples\test.jpg ^
  --output D:\results\single ^
  --top-k 3 ^
  --threshold 0.2
```

è¾“å‡ºä½äº `output\test_result.json`ï¼Œå†…å« Top-K é¢„æµ‹ç±»åˆ«åŠæ¦‚ç‡ã€‚

### æ‰¹é‡åˆ†ç±» + ç‰¹å¾ä¿å­˜

```powershell
python inference_artist.py ^
  --mode both ^
  --model lsnet_t_artist ^
  --checkpoint D:\experiments\lsnet_t\model_best.pth ^
  --class-csv D:\experiments\lsnet_t\class_mapping.csv ^
  --input D:\samples\batch ^
  --output D:\results\batch ^
  --batch-size 64 ^
  --top-k 3
```

å½“è¾“å…¥ä¸ºç›®å½•æ—¶ï¼š

- `batch_results.json`ï¼šé€å›¾åƒçš„åˆ†ç±»ç»“æœä¸ç‰¹å¾å‘é‡ï¼ˆæ”¯æŒtop-kå‚æ•°ï¼‰
- `features.npy`ï¼šå †å åçš„ç‰¹å¾çŸ©é˜µï¼Œå¯ç”¨äºèšç±»æˆ–ç›¸ä¼¼æ£€ç´¢
- `image_names.txt`ï¼šç‰¹å¾çŸ©é˜µçš„æ–‡ä»¶åé¡ºåº

### ä»…æå–ç‰¹å¾

```powershell
python inference_artist.py ^
  --mode cluster ^
  --model lsnet_t_artist ^
  --checkpoint D:\experiments\lsnet_t\model_best.pth ^
  --input D:\samples\batch ^
  --output D:\results\features_only
```

èšç±»æ¨¡å¼ä¸éœ€è¦ `--class-csv`ï¼Œåªéœ€æ¨¡å‹æƒé‡å³å¯è·å–ç‰¹å¾è¡¨ç¤ºã€‚

### è¿›é˜¶ï¼šæ‰¹é‡èšç±»ä¸å‘é‡ç›¸ä¼¼åº¦

**æå–å¹¶èšç±»ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼š**

```powershell
python tools/extract_cluster_features.py ^
  --images-dir D:\samples\artist_A ^
  --model lsnet_t_artist ^
  --checkpoint D:\experiments\lsnet_t\model_best.pth ^
  --num-clusters 6 ^
  --output-dir D:\results\artist_A_cluster
```

è¾“å‡ºï¼š

- `features.npy`ï¼šä¸Šè¿°ç›®å½•å†…å…¨éƒ¨å›¾åƒçš„ç‰¹å¾çŸ©é˜µ
- `cluster_assignments.json`ï¼šKMeans èšç±»ç»“æœï¼ŒåŒ…å«æ¯ä¸ªç°‡å†…çš„æ–‡ä»¶åã€è´¨å¿ƒä¸ç°‡å¤§å°

**æ¯”è¾ƒä¸€ä¸ªå‘é‡ä¸å¤šä¸ªå‚è€ƒå‘é‡çš„ç›¸ä¼¼åº¦ï¼š**

```powershell
python tools/compare_vectors.py ^
  --query-vector D:\results\query.npy ^
  --reference-vectors refs\vector1.npy refs\vector2.npy ^
  --top-k 3 ^
  --normalize ^
  --output D:\results\similarity.json
```

è„šæœ¬ä¼šå¯¹æŸ¥è¯¢å‘é‡ä¸å‚è€ƒå‘é‡æ‰§è¡Œä½™å¼¦ç›¸ä¼¼åº¦æ’åºï¼Œå¹¶åœ¨ç»ˆç«¯åŠå¯é€‰ JSON æ–‡ä»¶ä¸­ç»™å‡ºå‰ `top-k` ç»“æœã€‚`--reference-vectors` åŒæ—¶æ”¯æŒä¼ å…¥ç›®å½•ï¼ˆä¼šè‡ªåŠ¨è¯»å–å…¶ä¸­çš„ `.npy` æ–‡ä»¶ï¼‰ã€‚

## å¸¸è§é—®é¢˜æ’æŸ¥

| é—®é¢˜ | æ’æŸ¥å»ºè®® |
| --- | --- |
| `timm` å¯¼å…¥å¤±è´¥ | ç¡®è®¤å·²æ‰§è¡Œ `pip install -r requirements.txt`ï¼Œæˆ–æ‰‹åŠ¨å®‰è£… `pip install timm` |
| åˆ†ç±»æ¨ç†æç¤ºç¼ºå°‘ CSV | åˆ†ç±»æˆ– `both` æ¨¡å¼å¿…é¡»æä¾› `--class-csv`ï¼Œè¯·ä½¿ç”¨è®­ç»ƒè¾“å‡ºç›®å½•ä¸­çš„åŒåæ–‡ä»¶ |
| PyTorch 2.6 æ¢å¤è®­ç»ƒæŠ¥ `_pickle.UnpicklingError` | ä»£ç å·²åœ¨è®­ç»ƒè„šæœ¬ä¸­å…è®¸ `argparse.Namespace` ååºåˆ—åŒ–ï¼›è‹¥ä½¿ç”¨è‡ªå®šä¹‰è„šæœ¬ï¼Œè¯·åœ¨ `torch.load` å‰è°ƒç”¨ `torch.serialization.add_safe_globals([argparse.Namespace])`ï¼Œæˆ–æ˜¾å¼ä¼ å…¥ `weights_only=False` |
| æ•°æ®é›†åˆ’åˆ†è„šæœ¬è¦†ç›–æç¤º | è‹¥è¾“å‡ºç›®å½•å·²å­˜åœ¨ï¼Œéœ€è¦åœ¨æç¤ºåè¾“å…¥ `y` å…è®¸è¦†ç›– |
| Windows ä¸‹ç¬¦å·é“¾æ¥å¤±è´¥ | é»˜è®¤ä¸ºå¤åˆ¶æ¨¡å¼ï¼›è‹¥æƒ³ä½¿ç”¨ `--symlink` éœ€ä»¥ç®¡ç†å‘˜æ–¹å¼è¿è¡Œæˆ–ä¿æŒå¤åˆ¶ |

## é¡¹ç›®ç»“æ„é€Ÿè§ˆ

```
lsnet/
â”œâ”€â”€ train_artist_style.py      # è®­ç»ƒå…¥å£
â”œâ”€â”€ train_artist_multilabel.py  # å¤šæ ‡ç­¾è®­ç»ƒå…¥å£
â”œâ”€â”€ inference_artist.py        # æ¨ç†/ç‰¹å¾æå–è„šæœ¬
â”œâ”€â”€ predict_artist_multilabel.py # å¤šæ ‡ç­¾æ¨ç†è„šæœ¬
â”œâ”€â”€ tools/split_multilabel_dataset.py # è‡ªåŠ¨åˆ’åˆ†train/valå¹¶è¿‡æ»¤æ ‡ç­¾
â”œâ”€â”€ tools/generate_multilabel_csv.py # ç”±å›¾ç‰‡+txtç”Ÿæˆå¤šæ ‡ç­¾CSV
â”œâ”€â”€ prepare_dataset.py         # æ•°æ®é›†åˆ’åˆ†ä¸ CSV ç”Ÿæˆ
â”œâ”€â”€ model/                     # æ¨¡å‹å®šä¹‰
â”œâ”€â”€ data/                      # æ•°æ®å¢å¼ºä¸æ•°æ®é›†å®ç°ï¼ˆå« MultiLabelImageDatasetï¼‰
â”œâ”€â”€ utils.py / losses.py       # è®­ç»ƒå·¥å…·
â”œâ”€â”€ requirements.txt           # ä¾èµ–åˆ—è¡¨
â””â”€â”€ ...                        # å…¶ä»–æ€§èƒ½æµ‹è¯•ä¸è¯„ä¼°è„šæœ¬
```

## å¤§è§„æ¨¡è®­ç»ƒé…ç½® (100ä¸‡+å›¾ç‰‡ï¼Œ10ä¸‡+ç±»åˆ«)

å¯¹äºè¶…å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¨èä½¿ç”¨ä»¥ä¸‹é…ç½®ï¼š

### æ¨¡å‹é€‰æ‹©
- ä½¿ç”¨ `lsnet_xl_artist` æ¨¡å‹ï¼Œä¸“é—¨ä¸º10ä¸‡+ç±»åˆ«ä¼˜åŒ–
- ç‰¹å¾ç»´åº¦è®¾ç½®ä¸º2048æˆ–æ›´é«˜
- å¯ç”¨projectionå±‚ä»¥è·å¾—æ›´å¥½çš„ç‰¹å¾è¡¨ç¤º

### è®­ç»ƒå‚æ•°
```powershell
python train_artist_style.py ^
  --model lsnet_xl_artist ^
  --data-path D:\datasets\massive_artist_dataset ^
  --batch-size 64 ^       # æ¯GPUæ‰¹æ¬¡å¤§å°
  --accumulation-steps 4 ^ # æ¢¯åº¦ç´¯ç§¯ï¼Œç›¸å½“äº256çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
  --epochs 300 ^          # æ›´é•¿çš„è®­ç»ƒæ—¶é—´
  --lr 0.002 ^            # æ›´é«˜çš„å­¦ä¹ ç‡
  --weight-decay 0.1 ^    # æ›´å¤§çš„æƒé‡è¡°å‡
  --feature-dim 2048 ^    # æ›´å¤§çš„ç‰¹å¾ç»´åº¦
  --num_workers 16 ^      # æ›´å¤šæ•°æ®åŠ è½½è¿›ç¨‹
  --output-dir D:\experiments\massive_training ^
  --dist-eval              # å¯ç”¨åˆ†å¸ƒå¼è¯„ä¼°
```

### å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
```bash
# 8GPUè®­ç»ƒ (Linux/WSL)
torchrun --standalone --nnodes=1 --nproc_per_node=8 train_artist_style.py \
  --model lsnet_xl_artist \
  --data-path /mnt/d/datasets/massive_dataset \
  --batch-size 32 \
  --accumulation-steps 8 \
  --epochs 500 \
  --lr 0.003 \
  --weight-decay 0.15 \
  --feature-dim 4096 \
  --output-dir /mnt/d/experiments/massive_run
```

### å†…å­˜ä¼˜åŒ–å»ºè®®
- **æ¢¯åº¦ç´¯ç§¯**: ä½¿ç”¨ `--accumulation-steps N` æ¥æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡å¤§å°ï¼Œå‡å°‘æ˜¾å­˜å ç”¨
  - ä¾‹å¦‚ï¼š`--batch-size 32 --accumulation-steps 4` ç›¸å½“äºæœ‰æ•ˆæ‰¹æ¬¡å¤§å°128
  - å­¦ä¹ ç‡ä¼šè‡ªåŠ¨æ ¹æ®æœ‰æ•ˆæ‰¹æ¬¡å¤§å°è¿›è¡Œç¼©æ”¾
- å¯ç”¨ `--pin-mem` ä»¥åŠ é€Ÿæ•°æ®åŠ è½½
- ä½¿ç”¨ `--model-ema` è·å¾—æ›´ç¨³å®šçš„è®­ç»ƒ
- å®šæœŸä¿å­˜checkpointä»¥ä¾¿æ–­ç‚¹ç»­è®­
- ç›‘æ§GPUå†…å­˜ä½¿ç”¨ï¼Œå¿…è¦æ—¶å‡å°batch-size

- è‹¥éœ€é›†æˆåˆ° Web æœåŠ¡ï¼Œå¯å°† `inference_artist.py` å°è£…ä¸º APIï¼Œè¾“å‡º JSON ç»“æœæˆ–ç‰¹å¾åº“æŸ¥è¯¢ã€‚
- èšç±»æ¨¡å¼ç”Ÿæˆçš„ `features.npy` å¯ç›´æ¥æ¥å…¥ Faissã€Milvus ç­‰ç›¸ä¼¼åº¦æ£€ç´¢ç³»ç»Ÿã€‚
- å¦‚éœ€æ‰©å±•æ–°çš„ç”»å¸ˆç±»åˆ«ï¼Œé‡å¤æ‰§è¡Œâ€œæ•°æ®å‡†å¤‡ â†’ è®­ç»ƒ â†’ æ¨ç†â€æµç¨‹å³å¯ã€‚

# [LSNet: See Large, Focus Small](https://arxiv.org/abs/2503.23135)


Official PyTorch implementation of **LSNet**. CVPR 2025.

<p align="center">
  <img src="figures/throughput.svg" width=60%> <br>
  Models are trained on ImageNet-1K and the throughput
 is tested on a Nvidia RTX3090.
</p>

[LSNet: See Large, Focus Small](https://arxiv.org/abs/2503.23135).\
Ao Wang, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding\
[![arXiv](https://img.shields.io/badge/arXiv-2503.23135-b31b1b.svg)](https://arxiv.org/abs/2503.23135) [![Hugging Face Models](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue)](https://huggingface.co/jameslahm/lsnet/tree/main) [![Hugging Face Collection](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Collection-blue)](https://huggingface.co/collections/jameslahm/lsnet-67ebec0ab4e220e7918d9565)

We introduce LSNet, a new family of lightweight vision models inspired by dynamic heteroscale capability of the human visual system, i.e., "See Large, Focus Small". LSNet achieves state-of-the-art performance and efficiency trade-offs across various vision tasks.

<details>
  <summary>
  <font size="+1">Abstract</font>
  </summary>
Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a "See Large, Focus Small" strategy for lightweight vision network design. We introduce LS (<b>L</b>arge-<b>S</b>mall) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks.
</details>

## Classification on ImageNet-1K

### Models
- \* denotes the results with distillation.
- The throughput is tested on a Nvidia RTX3090 using [speed.py](./speed.py).

| Model | Top-1 | Params | FLOPs | Throughput | Ckpt | Log |
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| LSNet-T | 74.9 / 76.1* | 11.4M | 0.3G | 14708 | [T](https://huggingface.co/jameslahm/lsnet/blob/main/lsnet_t.pth) / [T*](https://huggingface.co/jameslahm/lsnet/blob/main/lsnet_t_distill.pth) | [T](logs/lsnet_t.log) / [T*](logs/lsnet_t_distill.log) |
| LSNet-S | 77.8 / 79.0* | 16.1M | 0.5G | 9023  | [S](https://huggingface.co/jameslahm/lsnet/blob/main/lsnet_s.pth) / [S*](https://huggingface.co/jameslahm/lsnet/blob/main/lsnet_s_distill.pth) | [S](logs/lsnet_s.log) / [S*](logs/lsnet_s_distill.log) |
| LSNet-B | 80.3 / 81.6* | 23.2M | 1.3G | 3996  | [B](https://huggingface.co/jameslahm/lsnet/blob/main/lsnet_b.pth) / [B*](https://huggingface.co/jameslahm/lsnet/blob/main/lsnet_b_distill.pth) | [B](logs/lsnet_b.log) / [B*](logs/lsnet_b_distill.log) |

## ImageNet  

### Prerequisites
`conda` virtual environment is recommended. 
```bash
conda create -n lsnet python=3.8
pip install -r requirements.txt
```

### Data preparation

Download and extract ImageNet train and val images from http://image-net.org/. The training and validation data are expected to be in the `train` folder and `val` folder respectively:
```
|-- /path/to/imagenet/
    |-- train
    |-- val
```

### Training
To train LSNet-T on an 8-GPU machine:
```bash
python -m torch.distributed.launch --nproc_per_node=8 --master_port 12345 --use_env main.py --model lsnet_t --data-path ~/imagenet --dist-eval
# For training with distillation, please add `--distillation-type hard`
# For LSNet-B, please add `--weight-decay 0.05`
```

### Testing 
```bash
python main.py --eval --model lsnet_t --resume ./pretrain/lsnet_t.pth --data-path ~/imagenet
```
Models can also be automatically downloaded from ğŸ¤— like below.
```python
import timm

model = timm.create_model(
    f'hf_hub:jameslahm/lsnet_{t/t_distill/s/s_distill/b/b_distill}',
    pretrained=True
)
```

## Downstream Tasks
[Object Detection and Instance Segmentation](detection/README.md)<br>
[Semantic Segmentation](segmentation/README.md)<br>
[Robustness Evaluation](README_robustness.md)

## Acknowledgement

Classification (ImageNet) code base is partly built with [EfficientViT](https://github.com/microsoft/Cream/tree/main/EfficientViT), [LeViT](https://github.com/facebookresearch/LeViT), [PoolFormer](https://github.com/sail-sg/poolformer) and [EfficientFormer](https://github.com/snap-research/EfficientFormer). 

The detection and segmentation pipeline is from [MMCV](https://github.com/open-mmlab/mmcv) ([MMDetection](https://github.com/open-mmlab/mmdetection) and [MMSegmentation](https://github.com/open-mmlab/mmsegmentation)). 

Thanks for the great implementations! 

## Citation

If our code or models help your work, please cite our paper:
```BibTeX
@misc{wang2025lsnetlargefocussmall,
      title={LSNet: See Large, Focus Small}, 
      author={Ao Wang and Hui Chen and Zijia Lin and Jungong Han and Guiguang Ding},
      year={2025},
      eprint={2503.23135},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.23135}, 
}
```