"""
è‡ªåŠ¨åˆ†å‰²ç”»å¸ˆæ•°æ®é›†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
æ”¯æŒä»ç®€å•çš„æ–‡ä»¶å¤¹ç»“æ„è‡ªåŠ¨åˆ›å»ºImageFolderæ ¼å¼çš„æ•°æ®é›†
"""
import argparse
import csv
import json
import random
import shutil
from pathlib import Path

from tqdm import tqdm


def get_args_parser():
    parser = argparse.ArgumentParser('Artist Dataset Split Tool', add_help=False)
    
    parser.add_argument('--source-dir', required=True, type=str,
                        help='æºæ•°æ®ç›®å½•ï¼ŒåŒ…å«å¤šä¸ªç”»å¸ˆæ–‡ä»¶å¤¹')
    parser.add_argument('--output-dir', default='./data/artist_dataset', type=str,
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--val-ratio', default=0.2, type=float,
                        help='éªŒè¯é›†æ¯”ä¾‹ (0.0-1.0)')
    parser.add_argument('--seed', default=42, type=int,
                        help='éšæœºç§å­')
    parser.add_argument('--min-images', default=10, type=int,
                        help='æ¯ä¸ªç”»å¸ˆæœ€å°‘å›¾åƒæ•°é‡ï¼Œå°‘äºæ­¤æ•°é‡çš„ç”»å¸ˆå°†è¢«è·³è¿‡')
    parser.add_argument('--copy', action='store_true', default=True,
                        help='å¤åˆ¶æ–‡ä»¶ï¼ˆé»˜è®¤ï¼‰')
    parser.add_argument('--symlink', action='store_true',
                        help='åˆ›å»ºç¬¦å·é“¾æ¥è€Œä¸æ˜¯å¤åˆ¶æ–‡ä»¶ï¼ˆèŠ‚çœç©ºé—´ï¼‰')
    parser.add_argument('--image-extensions', nargs='+', 
                        default=['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'],
                        help='æ”¯æŒçš„å›¾åƒæ‰©å±•å')
    
    return parser


def collect_artist_images(source_dir, image_extensions):
    """
    æ”¶é›†æ‰€æœ‰ç”»å¸ˆåŠå…¶å›¾åƒæ–‡ä»¶
    
    è¿”å›æ ¼å¼ï¼š
    {
        'artist_name': [image_path1, image_path2, ...]
    }
    """
    source_path = Path(source_dir)
    
    if not source_path.exists():
        raise ValueError(f"æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
    
    artist_images = {}
    
    # éå†æºç›®å½•ä¸‹çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹ï¼ˆæ¯ä¸ªå­æ–‡ä»¶å¤¹ä»£è¡¨ä¸€ä¸ªç”»å¸ˆï¼‰
    artist_dirs = [d for d in source_path.iterdir() if d.is_dir()]
    
    print(f"å‘ç° {len(artist_dirs)} ä¸ªç”»å¸ˆæ–‡ä»¶å¤¹")
    
    for artist_dir in tqdm(artist_dirs, desc="æ‰«æç”»å¸ˆæ–‡ä»¶å¤¹"):
        artist_name = artist_dir.name
        
        # æ”¶é›†è¯¥ç”»å¸ˆçš„æ‰€æœ‰å›¾åƒ
        images = []
        for ext in image_extensions:
            # æ”¯æŒå¤šçº§ç›®å½•
            images.extend(list(artist_dir.rglob(f"*{ext}")))
            images.extend(list(artist_dir.rglob(f"*{ext.upper()}")))
        
        if images:
            artist_images[artist_name] = images
    
    return artist_images


def split_dataset(artist_images, val_ratio, seed, min_images):
    """
    å°†æ¯ä¸ªç”»å¸ˆçš„å›¾åƒåˆ†å‰²ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    
    è¿”å›ï¼š
    train_split: {artist_name: [train_images]}
    val_split: {artist_name: [val_images]}
    """
    random.seed(seed)
    
    train_split = {}
    val_split = {}
    skipped_artists = []
    
    for artist_name, images in artist_images.items():
        # è¿‡æ»¤æ‰å›¾åƒæ•°é‡å¤ªå°‘çš„ç”»å¸ˆ
        if len(images) < min_images:
            skipped_artists.append((artist_name, len(images)))
            continue
        
        # éšæœºæ‰“ä¹±
        images_list = list(images)
        random.shuffle(images_list)
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        n_val = max(1, int(len(images_list) * val_ratio))  # è‡³å°‘1å¼ éªŒè¯å›¾åƒ
        n_train = len(images_list) - n_val
        
        # å¦‚æœè®­ç»ƒé›†å¤ªå°‘ï¼Œè°ƒæ•´åˆ†å‰²
        if n_train < 1:
            n_train = 1
            n_val = len(images_list) - 1
        
        train_split[artist_name] = images_list[:n_train]
        val_split[artist_name] = images_list[n_train:]
    
    if skipped_artists:
        print(f"\nâš ï¸  è·³è¿‡ {len(skipped_artists)} ä¸ªå›¾åƒæ•°é‡ä¸è¶³çš„ç”»å¸ˆï¼š")
        for artist, count in skipped_artists[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  - {artist}: {count} å¼ å›¾åƒ (æœ€å°‘éœ€è¦ {min_images} å¼ )")
        if len(skipped_artists) > 10:
            print(f"  ... è¿˜æœ‰ {len(skipped_artists) - 10} ä¸ª")
    
    return train_split, val_split


def create_dataset_structure(train_split, val_split, output_dir, use_symlink=False):
    """
    åˆ›å»ºImageFolderæ ¼å¼çš„æ•°æ®é›†ç»“æ„
    """
    output_path = Path(output_dir)
    train_dir = output_path / 'train'
    val_dir = output_path / 'val'
    
    # æ¸…ç†æ—§çš„è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    if output_path.exists():
        response = input(f"è¾“å‡ºç›®å½• {output_dir} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–ï¼Ÿ(y/n): ")
        if response.lower() == 'y':
            shutil.rmtree(output_path)
        else:
            print("å–æ¶ˆæ“ä½œ")
            return False
    
    # åˆ›å»ºç›®å½•ç»“æ„
    train_dir.mkdir(parents=True, exist_ok=True)
    val_dir.mkdir(parents=True, exist_ok=True)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_train_images = 0
    total_val_images = 0
    
    # å¤„ç†è®­ç»ƒé›†
    print("\nåˆ›å»ºè®­ç»ƒé›†...")
    for artist_name, images in tqdm(train_split.items(), desc="å¤„ç†ç”»å¸ˆ"):
        artist_train_dir = train_dir / artist_name
        artist_train_dir.mkdir(exist_ok=True)
        
        for img_path in images:
            dest_path = artist_train_dir / img_path.name
            
            # å¤„ç†æ–‡ä»¶åå†²çª
            counter = 1
            original_dest = dest_path
            while dest_path.exists():
                dest_path = original_dest.parent / f"{original_dest.stem}_{counter}{original_dest.suffix}"
                counter += 1
            
            if use_symlink:
                try:
                    dest_path.symlink_to(img_path.resolve())
                except OSError:
                    # Windowså¯èƒ½éœ€è¦ç®¡ç†å‘˜æƒé™ï¼Œå›é€€åˆ°å¤åˆ¶
                    shutil.copy2(img_path, dest_path)
            else:
                shutil.copy2(img_path, dest_path)
            
            total_train_images += 1
    
    # å¤„ç†éªŒè¯é›†
    print("\nåˆ›å»ºéªŒè¯é›†...")
    for artist_name, images in tqdm(val_split.items(), desc="å¤„ç†ç”»å¸ˆ"):
        artist_val_dir = val_dir / artist_name
        artist_val_dir.mkdir(exist_ok=True)
        
        for img_path in images:
            dest_path = artist_val_dir / img_path.name
            
            # å¤„ç†æ–‡ä»¶åå†²çª
            counter = 1
            original_dest = dest_path
            while dest_path.exists():
                dest_path = original_dest.parent / f"{original_dest.stem}_{counter}{original_dest.suffix}"
                counter += 1
            
            if use_symlink:
                try:
                    dest_path.symlink_to(img_path.resolve())
                except OSError:
                    shutil.copy2(img_path, dest_path)
            else:
                shutil.copy2(img_path, dest_path)
            
            total_val_images += 1
    
    return {
        'train_images': total_train_images,
        'val_images': total_val_images,
        'n_classes': len(train_split)
    }


def export_class_mapping_csv(class_to_idx, output_dir):
    """å¯¼å‡º class_id -> class_name æ˜ å°„ CSV"""
    csv_path = Path(output_dir) / 'class_mapping.csv'
    with csv_path.open('w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=['class_id', 'class_name'])
        writer.writeheader()
        for class_name, class_id in sorted(class_to_idx.items(), key=lambda x: x[1]):
            writer.writerow({'class_id': class_id, 'class_name': class_name})
    return csv_path


def save_dataset_info(train_split, val_split, output_dir, stats):
    """ä¿å­˜æ•°æ®é›†ä¿¡æ¯"""
    output_path = Path(output_dir)
    
    # ç±»åˆ«æ˜ å°„
    class_names = sorted(train_split.keys())
    class_to_idx = {name: idx for idx, name in enumerate(class_names)}
    
    # ä¿å­˜ç±»åˆ«åç§°ï¼ˆç”¨äºæ¨ç†æ—¶æ˜¾ç¤ºï¼‰
    class_names_file = output_path / 'class_names.json'
    with open(class_names_file, 'w', encoding='utf-8') as f:
        json.dump({str(idx): name for name, idx in class_to_idx.items()}, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    dataset_info = {
        'n_classes': len(class_names),
        'class_names': class_names,
        'class_to_idx': class_to_idx,
        'train_images': stats['train_images'],
        'val_images': stats['val_images'],
        'total_images': stats['train_images'] + stats['val_images'],
        'train_per_class': {name: len(images) for name, images in train_split.items()},
        'val_per_class': {name: len(images) for name, images in val_split.items()},
    }
    
    info_file = output_path / 'dataset_info.json'
    with open(info_file, 'w', encoding='utf-8') as f:
        json.dump(dataset_info, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜æ•°æ®åˆ†å‰²ä¿¡æ¯ï¼ˆç”¨äºå¤ç°ï¼‰
    split_file = output_path / 'split_info.txt'
    with open(split_file, 'w', encoding='utf-8') as f:
        f.write("è®­ç»ƒé›†:\n")
        for artist, images in sorted(train_split.items()):
            f.write(f"  {artist}: {len(images)} å¼ \n")
        
        f.write("\néªŒè¯é›†:\n")
        for artist, images in sorted(val_split.items()):
            f.write(f"  {artist}: {len(images)} å¼ \n")
    
    # å¯¼å‡º CSV ç±»åˆ«æ˜ å°„
    class_mapping_csv = export_class_mapping_csv(class_to_idx, output_dir)

    return class_names_file, info_file, split_file, class_mapping_csv


def print_summary(train_split, val_split, stats, output_dir):
    """æ‰“å°æ‘˜è¦ä¿¡æ¯"""
    print("\n" + "="*60)
    print("æ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
    print("="*60)
    
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  ç”»å¸ˆæ•°é‡: {stats['n_classes']}")
    print(f"  è®­ç»ƒé›†å›¾åƒ: {stats['train_images']}")
    print(f"  éªŒè¯é›†å›¾åƒ: {stats['val_images']}")
    print(f"  æ€»å›¾åƒæ•°: {stats['train_images'] + stats['val_images']}")
    
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"  â”œâ”€â”€ train/")
    print(f"  â”‚   â”œâ”€â”€ {list(train_split.keys())[0]}/ ({len(train_split[list(train_split.keys())[0]])} å¼ )")
    if len(train_split) > 1:
        print(f"  â”‚   â”œâ”€â”€ {list(train_split.keys())[1]}/ ({len(train_split[list(train_split.keys())[1]])} å¼ )")
    if len(train_split) > 2:
        print(f"  â”‚   â””â”€â”€ ... ({len(train_split) - 2} ä¸ªå…¶ä»–ç”»å¸ˆ)")
    print(f"  â”œâ”€â”€ val/")
    print(f"  â”‚   â”œâ”€â”€ {list(val_split.keys())[0]}/ ({len(val_split[list(val_split.keys())[0]])} å¼ )")
    if len(val_split) > 1:
        print(f"  â”‚   â””â”€â”€ ... ({len(val_split) - 1} ä¸ªå…¶ä»–ç”»å¸ˆ)")
    print(f"  â”œâ”€â”€ class_names.json")
    print(f"  â”œâ”€â”€ dataset_info.json")
    print(f"  â””â”€â”€ split_info.txt")
    
    print(f"\nğŸ¯ æ¯ä¸ªç”»å¸ˆçš„å›¾åƒåˆ†å¸ƒï¼ˆå‰10ä¸ªï¼‰:")
    for i, (artist, images) in enumerate(sorted(train_split.items())[:10]):
        train_count = len(images)
        val_count = len(val_split[artist])
        total = train_count + val_count
        print(f"  {i+1:2d}. {artist:30s} è®­ç»ƒ:{train_count:4d}  éªŒè¯:{val_count:4d}  æ€»è®¡:{total:4d}")
    
    if len(train_split) > 10:
        print(f"  ... è¿˜æœ‰ {len(train_split) - 10} ä¸ªç”»å¸ˆ")
    
    print(f"\nâœ… ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼")
    print(f"\nè®­ç»ƒå‘½ä»¤ç¤ºä¾‹:")
    print(f"python train_artist_style.py \\")
    print(f"    --model lsnet_t_artist \\")
    print(f"    --data-path {output_dir} \\")
    print(f"    --num-classes {stats['n_classes']} \\")
    print(f"    --batch-size 128 \\")
    print(f"    --epochs 300 \\")
    print(f"    --output-dir ./output/artist_model")


def main(args):
    print("="*60)
    print("ç”»å¸ˆæ•°æ®é›†è‡ªåŠ¨åˆ†å‰²å·¥å…·")
    print("="*60)
    
    print(f"\né…ç½®:")
    print(f"  æºç›®å½•: {args.source_dir}")
    print(f"  è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"  éªŒè¯é›†æ¯”ä¾‹: {args.val_ratio:.1%}")
    print(f"  æœ€å°‘å›¾åƒæ•°: {args.min_images}")
    print(f"  æ–‡ä»¶æ“ä½œ: {'ç¬¦å·é“¾æ¥' if args.symlink else 'å¤åˆ¶'}")
    print(f"  éšæœºç§å­: {args.seed}")
    
    # æ”¶é›†ç”»å¸ˆå›¾åƒ
    print(f"\næ­¥éª¤ 1/4: æ‰«ææºç›®å½•...")
    artist_images = collect_artist_images(args.source_dir, args.image_extensions)
    
    if not artist_images:
        print("âŒ é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶")
        print(f"   è¯·æ£€æŸ¥æºç›®å½•: {args.source_dir}")
        print(f"   æ”¯æŒçš„å›¾åƒæ ¼å¼: {', '.join(args.image_extensions)}")
        return
    
    total_images = sum(len(images) for images in artist_images.values())
    print(f"âœ“ æ‰¾åˆ° {len(artist_images)} ä¸ªç”»å¸ˆï¼Œå…± {total_images} å¼ å›¾åƒ")
    
    # åˆ†å‰²æ•°æ®é›†
    print(f"\næ­¥éª¤ 2/4: åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†...")
    train_split, val_split = split_dataset(
        artist_images, 
        args.val_ratio, 
        args.seed, 
        args.min_images
    )
    
    if not train_split:
        print("âŒ é”™è¯¯ï¼šæ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œåˆ†å‰²")
        print(f"   æ¯ä¸ªç”»å¸ˆè‡³å°‘éœ€è¦ {args.min_images} å¼ å›¾åƒ")
        return
    
    print(f"âœ“ åˆ†å‰²å®Œæˆï¼š{len(train_split)} ä¸ªç”»å¸ˆï¼Œè®­ç»ƒ/éªŒè¯æ¯”ä¾‹ = {1-args.val_ratio:.0%}/{args.val_ratio:.0%}")
    
    # åˆ›å»ºæ•°æ®é›†ç»“æ„
    print(f"\næ­¥éª¤ 3/4: åˆ›å»ºæ•°æ®é›†ç›®å½•ç»“æ„...")
    stats = create_dataset_structure(
        train_split, 
        val_split, 
        args.output_dir, 
        use_symlink=args.symlink
    )
    
    if not stats:
        return
    
    print(f"âœ“ æ•°æ®é›†ç»“æ„åˆ›å»ºå®Œæˆ")
    
    # ä¿å­˜å…ƒä¿¡æ¯
    print(f"\næ­¥éª¤ 4/4: ä¿å­˜æ•°æ®é›†ä¿¡æ¯...")
    class_names_file, info_file, split_file, class_mapping_csv = save_dataset_info(
        train_split, 
        val_split, 
        args.output_dir, 
        stats
    )
    
    print(f"âœ“ ä¿¡æ¯æ–‡ä»¶å·²ä¿å­˜")
    print(f"  - {class_names_file}")
    print(f"  - {info_file}")
    print(f"  - {split_file}")
    print(f"  - {class_mapping_csv}")
    
    # æ‰“å°æ‘˜è¦
    print_summary(train_split, val_split, stats, args.output_dir)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        'Artist Dataset Split Tool', 
        parents=[get_args_parser()],
        description='è‡ªåŠ¨å°†ç”»å¸ˆæ–‡ä»¶å¤¹åˆ†å‰²ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†'
    )
    args = parser.parse_args()
    main(args)
